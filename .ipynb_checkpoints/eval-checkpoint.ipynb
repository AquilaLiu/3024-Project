{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-26T09:40:33.991495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchvision import datasets, transforms\n",
    "CONFIG = {\n",
    "    \"image_size\": 32,\n",
    "    \"batch_size_train\": 32,\n",
    "    \"batch_size_eval\": 32,\n",
    "    \"num_epochs\": 5,  # \"training loops\"\n",
    "    \"lr\": 1e-4,\n",
    "    \"timesteps\": 1000,\n",
    "    \"cfg_scale\": 7.5,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG[\"image_size\"], CONFIG[\"image_size\"])),\n",
    "    transforms.ToTensor(),                      # [0,1]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # => [-1,1]\n",
    "])\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size_train\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size_eval\"], shuffle=False)\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=CONFIG[\"timesteps\"],\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    "    beta_schedule=\"linear\"\n",
    ")\n",
    "class ClassConditionedUNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, image_size=32):\n",
    "        super().__init__()\n",
    "        self.label_dropout_prob = 0.1\n",
    "        self.null_class_id = num_classes\n",
    "        self.unet = UNet2DModel(\n",
    "            sample_size=image_size,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            layers_per_block=2,\n",
    "            block_out_channels=(128, 256, 256),\n",
    "            down_block_types=(\n",
    "                \"DownBlock2D\", \n",
    "                \"AttnDownBlock2D\",\n",
    "                \"DownBlock2D\"\n",
    "            ),\n",
    "            up_block_types=(\n",
    "                \"UpBlock2D\", \n",
    "                \"AttnUpBlock2D\",\n",
    "                \"UpBlock2D\"\n",
    "            )\n",
    "            ,\n",
    "            class_embed_type=\"timestep\",\n",
    "            num_class_embeds=num_classes + 1,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t, class_labels=None):\n",
    "        if class_labels is None:\n",
    "            # In eval, caller should provide labels (even for unconditional)\n",
    "            raise ValueError(\"class_labels must be provided.\")\n",
    "        \n",
    "        if self.training:\n",
    "            # Apply label dropout: replace some labels with null_class_id\n",
    "            drop_mask = torch.rand(class_labels.shape[0], device=x.device) < self.label_dropout_prob\n",
    "            class_labels = class_labels.clone()\n",
    "            class_labels[drop_mask] = self.null_class_id  # e.g., 10\n",
    "        \n",
    "        return self.unet(x, t, class_labels=class_labels).sample\n"
   ],
   "id": "b8ff2c5c0489b28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate",
   "id": "3109522088efa3c3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "model = ClassConditionedUNet(num_classes=10, image_size=CONFIG[\"image_size\"])\n",
    "model.load_state_dict(torch.load('mnist_diffusion_model.pth'))\n",
    "model=model.to(CONFIG[\"device\"])\n",
    "\n",
    "def to_rgb_grayscale(tensor):\n",
    "    \"\"\"Convert (N, 1, H, W) grayscale [0,1] to (N, 3, H, W) RGB for Inception metrics.\"\"\"\n",
    "    return tensor.repeat(1, 3, 1, 1)  # replicate channel 3 times\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_samples(model, noise_scheduler, num_samples=1000, guidance_scale=7.5, \n",
    "                     num_classes=10, image_size=32, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate samples using Classifier-Free Guidance (CFG).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Random class labels (uniform over 0‚Äì9)\n",
    "    class_labels = torch.randint(0, num_classes, (num_samples,), device=device)\n",
    "    \n",
    "    # Start from pure noise (32x32 grayscale)\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    latents = torch.randn(\n",
    "        num_samples, 1, image_size, image_size,\n",
    "        generator=generator, device=device\n",
    "    )\n",
    "\n",
    "    # Denoising loop\n",
    "    for t in tqdm(noise_scheduler.timesteps, desc=\"Sampling\"):\n",
    "        # CFG: duplicate latents for unconditional + conditional\n",
    "        latent_model_input = torch.cat([latents] * 2)  # (2*B, 1, H, W)\n",
    "        latent_model_input = noise_scheduler.scale_model_input(latent_model_input, t)\n",
    "        uncond_latents = latent_model_input[:num_samples]\n",
    "        cond_latents = latent_model_input[num_samples:]\n",
    "        null_labels = torch.full_like(class_labels, model.null_class_id)\n",
    "        noise_pred_uncond = model(uncond_latents, t, class_labels=null_labels)\n",
    "        noise_pred_cond = model(cond_latents, t, class_labels=class_labels)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "        latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "    samples = (latents.clamp(-1, 1) + 1) / 2\n",
    "    return samples, class_labels\n",
    "\n",
    "def evaluate(model, noise_scheduler, test_loader, device, num_samples=1):\n",
    "    print(\"\\nüîç Starting Evaluation...\")\n",
    "    \n",
    "    # === 1. Generate samples ===\n",
    "    print(f\"üé® Generating {num_samples} samples with CFG scale = {CONFIG['cfg_scale']}...\")\n",
    "    gen_samples, gen_labels = generate_samples(\n",
    "        model=model,\n",
    "        noise_scheduler=noise_scheduler,\n",
    "        num_samples=num_samples,\n",
    "        guidance_scale=CONFIG[\"cfg_scale\"],\n",
    "        num_classes=10,\n",
    "        image_size=CONFIG[\"image_size\"],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # === 2. Prepare real test images (same count, normalized [0,1]) ===\n",
    "    print(\"üì¶ Loading real test images...\")\n",
    "    real_images = []\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        # Denormalize [-1,1] ‚Üí [0,1]\n",
    "        images = (images + 1) / 2\n",
    "        real_images.append(images)\n",
    "        if len(real_images) * CONFIG[\"batch_size_eval\"] >= num_samples:\n",
    "            break\n",
    "    real_images = torch.cat(real_images, dim=0)[:num_samples]  # (N, 1, H, W)\n",
    "    save_sample_grid(gen_samples[:64], \"generated_samples.png\",nrow=1)\n",
    "    # === 3. Prepare for Inception metrics: grayscale ‚Üí RGB ===\n",
    "    gen_rgb = to_rgb_grayscale(gen_samples)  # (N, 3, H, W), [0,1]\n",
    "    real_rgb = to_rgb_grayscale(real_images)  # (N, 3, H, W), [0,1]\n",
    "    \n",
    "    # Ensure tensors are float and on same device\n",
    "    gen_rgb = gen_rgb.float()\n",
    "    real_rgb = real_rgb.float()\n",
    "    \n",
    "    # === 4. Compute Inception Score (IS) ===\n",
    "    print(\"Computing Inception Score...\")\n",
    "    is_metric = InceptionScore(normalize=True).to(device)\n",
    "    is_metric.update(gen_rgb)\n",
    "    is_mean, is_std = is_metric.compute()\n",
    "    \n",
    "    # === 5. Compute Fr√©chet Inception Distance (FID) ===\n",
    "    print(\"Computing FID...\")\n",
    "    fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n",
    "    fid_metric.update(real_rgb, real=True)\n",
    "    fid_metric.update(gen_rgb, real=False)\n",
    "    fid_score = fid_metric.compute()\n",
    "    \n",
    "    # === 6. Log & Print ===\n",
    "    metrics = {\n",
    "        \"inception_score_mean\": is_mean.item(),\n",
    "        \"inception_score_std\": is_std.item(),\n",
    "        \"fid_score\": fid_score.item()\n",
    "    }\n",
    "    \n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"   Inception Score: {is_mean:.4f} ¬± {is_std:.4f}\")\n",
    "    print(f\"   FID: {fid_score:.4f}\")\n",
    "    \n",
    "    # Optional: Save sample grid\n",
    "    save_sample_grid(gen_samples[:64], \"generated_samples.png\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def save_sample_grid(samples, filename, nrow=8):\n",
    "    \"\"\"Save a grid of generated samples (64 images, 8x8).\"\"\"\n",
    "    from torchvision.utils import make_grid\n",
    "    grid = make_grid(samples[:64], nrow=nrow, padding=2, normalize=True, value_range=(0, 1))\n",
    "    grid_img = transforms.ToPILImage()(grid.cpu())\n",
    "    grid_img.save(filename)\n",
    "    print(f\"üñºÔ∏è Sample grid saved as '{filename}' and logged to SwanLab.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Run Evaluation\n",
    "# ----------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ Running Final Evaluation\")\n",
    "print(\"=\"*50)\n",
    "model.eval()\n",
    "eval_metrics = evaluate(\n",
    "    model=model,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    test_loader=test_loader,\n",
    "    device=CONFIG[\"device\"],\n",
    "    num_samples=1000  # standard for IS/FID\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
